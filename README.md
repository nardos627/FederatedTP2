# Federated Learning: Handling Data Heterogeneity and Client Drift

![Github Repo](https://github.com/nardos627/FederatedTP2) 


## üìù Description
Experimental analysis of federated learning algorithms (FedAvg, FedProx, SCAFFOLD) under varying data heterogeneity conditions (Œ±=10, 1.0, 0.1). Investigates client drift mitigation and comparative algorithm performance.

## üöÄ Quick Start Commands

### Server Execution
```bash
# Standard FedAvg
python main.py run-server --strategy fedavg --rounds 50

# FedProx with Œº=0.1
python main.py run-server --strategy fedprox --mu 0.1 --rounds 50

# SCAFFOLD implementation
python main.py run-server --strategy scaffold --rounds 50

# FedAvg client
python main.py run-client --cid=0 --strategy=fedavg

# FedProx client 
python main.py run-client --cid=0 --strategy=fedprox

# SCAFFOLD client
python main.py run-client --cid=0 --strategy=scaffold

## üìä Experimental Results

### Accuracy by Heterogeneity Level
| Algorithm   | Œ±=10 (Near-IID) | Œ±=1.0 (Moderate) | Œ±=0.1 (High) |
|-------------|-----------------|------------------|--------------|
| FedAvg      | 85%             | 70%              | 55%          |
| FedProx     | 84%             | 79%              | 71%          |
| SCAFFOLD    | 86%             | 81%              | 78%          |

### Performance Characteristics
- **Convergence Speed**: SCAFFOLD (Fastest) > FedProx > FedAvg (Slowest)
- **Client Drift Resistance**: SCAFFOLD (Best) > FedProx > FedAvg
- **Computation Cost**: SCAFFOLD (Highest) > FedProx > FedAvg (Lowest)

## üîç Key Findings
- SCAFFOLD dominates across all heterogeneity levels (+5-23% over FedAvg)
- FedProx most effective at Œº=0.1 (optimal regularization)
- FedAvg only suitable for near-IID data (Œ±‚â•1.0)
